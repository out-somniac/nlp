{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29cab7ec",
   "metadata": {},
   "source": [
    "\n",
    "# **Laboratorium 002: Skuteczne promptowanie modeli LLM w Ollama**\n",
    "\n",
    "## **Cel zajęć**\n",
    "\n",
    "- Wyćwiczenie praktycznych technik **prompt engineeringu** na lokalnych modelach uruchamianych przez **Ollama**.  \n",
    "- Zastosowanie **struktur i ról** (system/developer, user, assistant) w praktyce – z naciskiem na *kompozycję promptów*, a nie pojedyncze pytania.  \n",
    "- Przeprowadzenie **ewaluacji promptów** (PromptFoo) i analizy zachowania modeli.  \n",
    "- Zrozumienie zasad **bezpieczeństwa, prompt injection, anonimizacji** i dobrych praktyk projektowych.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Przygotowanie środowiska i prerekwizyty**\n",
    "\n",
    "- Zainstalowana **Ollama** i dwa modele w konwencji *polski – niepolski* (`llama3.1:8b`, `gemma2:2b`, `SpeakLeash/bielik-*`).  \n",
    "\n",
    "Test działania modelu:\n",
    "\n",
    "```bash\n",
    "echo \"Napisz rymowankę o najlepszym wykładowcy na WI.\" | ollama run gemma2:2b\n",
    "```\n",
    "---\n",
    "\n",
    "- Możemy też przenieść się z pracą do notebooka, wtedy nasz warsztat będzie wyglądał tak:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189efaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='gemma2:2b',\n",
    "    messages=[{'role': 'user', 'content': 'Napisz rymowankę o najlepszym wykładowcy na WI'}]\n",
    ")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee97015f",
   "metadata": {},
   "source": [
    "## **2. Struktura i techniki promptowania**\n",
    "\n",
    "### 2.1 Zero-shot\n",
    "\n",
    "**Zero‑shot prompting** to sposób formułowania zapytania do dużego modelu językowego, w którym nie podajemy mu ani jednego przykładu wykonanego zadania. Model dostaje wyłącznie instrukcję („zrób X”) i całą resztę dedukuje sam na podstawie wiedzy nabytej w treningu.\n",
    "\n",
    "Zadania, do których używa się **zero-shot prompting**:\n",
    "1. Szybkie prototypowanie – gdy chcemy sprawdzić, czy model może poradzić sobie z danym zadaniem (czy jego architektura i materiał treningowy na to pozwalają).\n",
    "\n",
    "2. Zadania ogólne o prostym formacie wyjściowym (tłumaczenia, parafrazy, proste klasyfikacje).\n",
    "\n",
    "3. Systemy low-code/no-code – użytkownik pisze naturalnym językiem, a model odpowiada bez konfiguracji.\n",
    "\n",
    "4. Ekstrakcja informacji _ad hoc_ (imię, mail, daty z tekstu itp.) tam, gdzie nie opłaca się trenować lub fine-tune'ować modelu.\n",
    "\n",
    "\n",
    "Kiedy **zero-shot** jest najbardziej efektywne?\n",
    "- brak lub szczątkowe dane oznaczone – nie mamy przykładowych par wejście→wyjście.\n",
    "- zadanie jest podobne do scenariuszy widzianych w treningu (np. „podsumuj”, „przetłumacz”, „odpowiedz TAK/NIE”).\n",
    "- liczy się czas wdrożenia – chcemy odpowiedzi „tu i teraz”, bez skrupulatnego modyfikowania promptu.\n",
    "\n",
    "Kiedy nie wystarcza?\n",
    "- złożone transformacje wymagające ścisłej struktury lub niestandardowych reguł.\n",
    "- zadania domenowe z mało popularnym żargonem (medycyna, prawo) – wtedy lepszy jest few-shot lub RAG.\n",
    "\n",
    "```bash\n",
    "echo \"Wypisz 5 słów kluczowych z notatki o spotkaniu Koła Naukowego BIT.\" | ollama run gemma2:2b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a383c",
   "metadata": {},
   "source": [
    "🔵 Spotkanie Koła Naukowego BIT\n",
    "\n",
    "📅 Środa, godz. 14:00\n",
    "📍 Sala 4.30D, Wydział Informatyki\n",
    "\n",
    "Serdecznie zapraszamy wszystkich studentów — zarówno obecnych członków, jak i osoby, które dopiero chcą dołączyć — na kolejne spotkanie Koła Naukowego BIT!\n",
    "\n",
    "💡 Plan spotkania:\n",
    "\n",
    "- Omówienie planu projektów na semestr zimowy – propozycje tematów z zakresu AI, bezpieczeństwa i tworzenia aplikacji webowych.\n",
    "\n",
    "- Prezentacja aktualnych inicjatyw – praca nad systemem rozpoznawania obrazu oraz chatbotem opartym na modelach LLM.\n",
    "\n",
    "- Warsztaty wprowadzające – krótkie zajęcia praktyczne z narzędzia Ollama i prompt engineeringu.\n",
    "\n",
    "- Podział na zespoły projektowe – możliwość zapisania się do wybranego projektu.\n",
    "\n",
    " - Dyskusja i networking – poznaj ludzi, którzy programują z pasją!\n",
    "\n",
    "🚀 Dlaczego warto dołączyć?\n",
    "\n",
    "- Rozwijasz praktyczne umiejętności techniczne (AI, ML, web, backend).\n",
    "\n",
    "- Bierzesz udział w projektach badawczo-rozwojowych i hackathonach.\n",
    "\n",
    "- Zyskujesz doświadczenie zespołowe i wsparcie mentorów z branży.\n",
    "\n",
    "- Spotykasz ludzi, którzy chcą robić coś więcej niż tylko zaliczać przedmioty.\n",
    "\n",
    "📢 Nie musisz mieć doświadczenia — wystarczy ciekawość i chęć nauki!\n",
    "Przyjdź w środę o 14:00 do sali 4.30D i zobacz, czym żyje nasze koło!\n",
    "\n",
    "✉️ W razie pytań: bit@wi.edu.pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd0875",
   "metadata": {},
   "source": [
    "Zadanie 2.1.1 Generowanie streszczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08241491",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "🔵 Spotkanie Koła Naukowego BIT\n",
    "\n",
    "📅 Środa, godz. 14:00\n",
    "📍 Sala 4.30D, Wydział Informatyki\n",
    "\n",
    "Serdecznie zapraszamy wszystkich studentów — zarówno obecnych członków, jak i osoby, które dopiero chcą dołączyć — na kolejne spotkanie Koła Naukowego BIT!\n",
    "\n",
    "💡 Plan spotkania:\n",
    "\n",
    "Omówienie planu projektów na semestr zimowy – propozycje tematów z zakresu AI, bezpieczeństwa i tworzenia aplikacji webowych.\n",
    "\n",
    "Prezentacja aktualnych inicjatyw – praca nad systemem rozpoznawania obrazu oraz chatbotem opartym na modelach LLM.\n",
    "\n",
    "Warsztaty wprowadzające – krótkie zajęcia praktyczne z narzędzia Ollama i prompt engineeringu.\n",
    "\n",
    "Podział na zespoły projektowe – możliwość zapisania się do wybranego projektu.\n",
    "\n",
    "Dyskusja i networking – poznaj ludzi, którzy programują z pasją!\n",
    "\n",
    "🚀 Dlaczego warto dołączyć?\n",
    "\n",
    "Rozwijasz praktyczne umiejętności techniczne (AI, ML, web, backend).\n",
    "\n",
    "Bierzesz udział w projektach badawczo-rozwojowych i hackathonach.\n",
    "\n",
    "Zyskujesz doświadczenie zespołowe i wsparcie mentorów z branży.\n",
    "\n",
    "Spotykasz ludzi, którzy chcą robić coś więcej niż tylko zaliczać przedmioty.\n",
    "\n",
    "📢 Nie musisz mieć doświadczenia — wystarczy ciekawość i chęć nauki!\n",
    "Przyjdź w środę o 14:00 do sali 4.30D i zobacz, czym żyje nasze koło!\n",
    "\n",
    "✉️ W razie pytań: bit@wi.edu.pl\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Return a JSON with one key \"summary\" containing\n",
    "a 30‑word English summary of the following text:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='gemma2:2b',\n",
    "    messages=[{'role': 'user', 'content': prompt}]\n",
    ")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2507c02",
   "metadata": {},
   "source": [
    "Zadanie 2.1.2 Otrzymywanie odpowiedzi od modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2a76f6",
   "metadata": {},
   "source": [
    "Poprzednie laboratoria były poświęcone głównie zapytaniom typu zero-shot, którymi weryfikuje się działanie modeli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17301812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytanie weryfikujące wiedzę i pewność modelu\n",
    "response = ollama.chat(\n",
    "    model='gemma2:2b',\n",
    "    messages=[{'role': 'user', 'content': 'jaka jest pogoda w warszawie'}]\n",
    ")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2238bf15",
   "metadata": {},
   "source": [
    "Zadanie 2.1.3\n",
    "\n",
    "Używając odpowiednich technik promptowania, możemy wymusić odpowiedź na modelu. Posłużmy się więc powyższym przykładem i rozbudujmy odpowiedź modelu, wykorzystując schemat JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d0e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class CityWeather(BaseModel):\n",
    "    city: str\n",
    "    temp_c: float\n",
    "    condition: str | None\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='gemma2:2b',\n",
    "    messages=[{'role':'user','content':'Weather in Warsaw today.'}],\n",
    "    format=CityWeather.model_json_schema(),   # ← schema trafia do Ollamy\n",
    "    options={'temperature':0}\n",
    ")\n",
    "\n",
    "weather = CityWeather.model_validate_json(response['message']['content'])\n",
    "print(weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332d73b3",
   "metadata": {},
   "source": [
    "Podobne pytanie w PowerShellu:\n",
    "```PowerShell\n",
    "@'\n",
    "<ROLE>Jesteś asystentem danych. Zwracasz tylko JSON.</ROLE>\n",
    "<GOAL>Wyodrębnij dane liczbowe i jednostki.</GOAL>\n",
    "<RULES>- Jeśli brak danych, zwróć null.</RULES>\n",
    "<INPUT>Jan kupił 3 jabłka po 2 zł.</INPUT>\n",
    "<OUTPUT>\n",
    "'@ | ollama run gemma2:2b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7aff2e",
   "metadata": {},
   "source": [
    "> **Pytania pomocnicze**  \n",
    "> - Jakiej odpowiedzi oczekiwałeś/aś?  \n",
    "> - Czy model zachował format JSON?  \n",
    "> - W jaki inny sposób można wymusić przestrzeganie formatu?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf59b95",
   "metadata": {},
   "source": [
    "Dodatkowe wskazówki:\n",
    "\n",
    "1. Nie otaczaj JSON-u blokiem json – przy format=\"json\" to zbędne, a istnieje ryzyko, że model uzna ``` za legalny token i walidacja się przerwie.\n",
    "\n",
    "2. Daj modelowi „awaryjną” ścieżkę – np. If you can’t comply, output {}. Zmniejsza szansę na halucynacje.\n",
    "\n",
    "3. Limit długości – przy dużych strukturach dodaj max_tokens, żeby model nie wypadł poza kontekst i nie zamknął nawiasu.\n",
    "\n",
    "4. Testuj → parsuj → próbuj od nowa – w kodzie API zawsze parsuj JSON i w razie JSONDecodeError ponawiaj z tym samym promptem; przy format='json' błędy i tak zdarzają się rzadko.\n",
    "\n",
    "5. Użytkownik-czat – gdy korzystasz interaktywnie (bez API), nie masz formatu; wtedy najpewniejszy miks to system-instrukcja + <json>…</json> + temp=0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb97a47",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.2 Few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27edcb5e",
   "metadata": {},
   "source": [
    "Dodając **przykłady** w promptach (na różnych poziomach), wskazujemy modelowi pożądany wzorzec.\n",
    "\n",
    "Zadanie 2.2.1  \n",
    "Przygotuj co najmniej dwa przykłady (pytanie → odpowiedź) i poproś model o wygenerowanie odpowiedzi dla nowego pytania w tym samym stylu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a762d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"role\": \"user\", \"content\": \"Translate to emoji: I love programming\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"💻❤️\"},\n",
    "    {\"role\": \"user\", \"content\": \"Translate to emoji: Fire and ice\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"🔥❄️\"},\n",
    "    {\"role\": \"user\", \"content\": \"Translate to emoji: Peace and coffee. Trophy and fast car. Running and winning.\"}\n",
    "]\n",
    "\n",
    "response = ollama.chat(model='gemma2:2b', messages=examples)\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb500f5",
   "metadata": {},
   "source": [
    "Zadanie 2.2.2 Ekstrakcja danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"role\": \"user\", \"content\": 'Wejście: \"Spotkanie w środę o 14:00 w sali 4.30D.\"'},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"data\":\"środa\",\"godzina\":\"14:00\",\"miejsce\":\"sala 4.30D\"}'},\n",
    "    {\"role\": \"user\", \"content\": 'Wejście: \"Jutro o 9:30 na Teamsach.\"'},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"data\":\"jutro\",\"godzina\":\"09:30\",\"miejsce\":\"Teams\"}'},\n",
    "    {\"role\": \"user\", \"content\": 'Wejście: \"W czwartek po południu w Warszawie, kawiarnia Relaks.\"'}\n",
    "]\n",
    "\n",
    "response = ollama.chat(model='gemma2:2b', messages=examples)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064211f",
   "metadata": {},
   "source": [
    "Podobna struktura zapytań w PowerShellu:\n",
    "```bash\n",
    "\n",
    "@'\n",
    "Przykład 1:\n",
    "Wejście: \"Spotkanie w środę o 14:00 w sali 4.30D.\"\n",
    "Wyjście: {\"data\":\"środa\",\"godzina\":\"14:00\",\"miejsce\":\"sala 4.30D\"}\n",
    "\n",
    "Przykład 2:\n",
    "Wejście: \"Jutro o 9:30 na Teamsach.\"\n",
    "Wyjście: {\"data\":\"jutro\",\"godzina\":\"09:30\",\"miejsce\":\"Teams\"}\n",
    "\n",
    "Nowe wejście: \"W czwartek po południu w Warszawie, kawiarnia Relaks.\"\n",
    "'@ | ollama run llama3.1:8b\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa0809",
   "metadata": {},
   "source": [
    "Zadanie 2.2.3 Ekstrakcja danych do formatu JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2744a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "schema = \"\"\"Return valid JSON: { \"name\": <string>, \"title\": <string>, \"company\": <string> }\"\"\"\n",
    "\n",
    "shots = [\n",
    "    # przykład 1\n",
    "    {\"role\": \"user\",      \"content\": schema + \"\\n— Sarah Connors-Newman — Director of Operations at Skynet Industries\"},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"name\":\"Sarah Connors-Newman\",\"title\":\"Director of Operations\",\"company\":\"Skynet Industries\"}'},\n",
    "    # przykład 2\n",
    "    {\"role\": \"user\",      \"content\": schema + \"\\n— Dr. Hiro Tanaka, Lead Scientist • QuantumX\"},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"name\":\"Hiro Tanaka\",\"title\":\"Lead Scientist\",\"company\":\"QuantumX\"}'},\n",
    "    # przykład 3\n",
    "    {\"role\": \"user\",      \"content\": schema + \"\\n— Prof. dr hab. Janek Dzbanek Przewodnik Wszystkich AGH\"},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"name\":\"Janek Dzbanek\",\"title\":\"Przewodnik Wszystkich\",\"company\":\"AGH\"}'},\n",
    "]\n",
    "\n",
    "text = \"• Prof. dr hab. Janek Tanaka wysadził AGH w powietrze\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"gemma2:2b\",\n",
    "    messages=shots + [\n",
    "        {\"role\": \"user\", \"content\": schema + \"\\n\" + text}\n",
    "    ],\n",
    "    format=\"json\",              \n",
    "    options={\"temperature\": 0}\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"message\"][\"content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9893b3",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Role i wiadomości systemowe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b50a3",
   "metadata": {},
   "source": [
    "W Ollama (jak w OpenAI) możemy dodać komunikat **`system`**, który definiuje rolę lub ograniczenia modelu.\n",
    "\n",
    "Zadanie 3.1 Definiowanie roli\n",
    "\n",
    "Zdefiniuj rolę **nauczyciela języka polskiego** i poproś o poprawienie błędów w zdaniu ucznia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c01833",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a strict but encouraging Polish language teacher.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Popraw proszę to zdanie: 'Wczoraj byłem w kinie i oglądali film.'\"}\n",
    "]\n",
    "\n",
    "response = ollama.chat(model='gemma2:2b', messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72af63d5",
   "metadata": {},
   "source": [
    "## 4. Chain‑of‑Thought (CoT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9226a27f",
   "metadata": {},
   "source": [
    "**Chain‑of‑Thought** zachęca model do wypisywania kroków rozumowania.\n",
    "\n",
    "Zadanie 4.1 Wyjaśnianie kroków rozumowania\n",
    "\n",
    "Poproś model, aby rozwiązał zagadkę logiczną i podał kroki rozumowania, a na końcu hasło w linii `ANSWER: ...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3273d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "puzzle = \"\"\"Jestem liczbą dwucyfrową. Moja suma cyfr to 9, a po odwróceniu cyfr jestem o 27 mniejsza. Jaką liczbą jestem?\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Let's solve this step‑by‑step.\n",
    "\n",
    "{puzzle}\n",
    "\n",
    "Format:\n",
    "STEP 1: ...\n",
    "STEP 2: ...\n",
    "ANSWER: <number>\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(model='gemma2:2b', messages=[{'role':'user','content':prompt}])\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7189b4",
   "metadata": {},
   "source": [
    "Zadanie 4.2 Wyjaśnianie kroków rozumowania z przypisanymi rolami systemowymi\n",
    "\n",
    "Poproś model, by rozwiązał zagadkę logiczną. Rozpisz zadania dla poszczególnych elementów systemu.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be41eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "puzzle = (\n",
    "    \"Jestem liczbą dwucyfrową. Moja suma cyfr to 9, \"\n",
    "    \"a po odwróceniu cyfr jestem o 27 mniejsza. Jaką liczbą jestem?\"\n",
    ")\n",
    "\n",
    "SYSTEM = (\n",
    "    \"You are a careful math tutor. \"\n",
    "    \"Explain EVERY algebraic step explicitly, without skipping or merging steps. \"\n",
    "    \"Never combine two operations into one line; label them as separate STEP n.\"\n",
    ")\n",
    "\n",
    "USER = f\"\"\"\n",
    "Solve the puzzle **step-by-step**.\n",
    "\n",
    "{puzzle}\n",
    "\n",
    "Output format (exactly):\n",
    "STEP 1: …\n",
    "STEP 2: …\n",
    "…\n",
    "ANSWER: <number>\n",
    "Do NOT skip, abbreviate, or summarise any step.\n",
    "\"\"\"\n",
    "\n",
    "resp = ollama.chat(\n",
    "    model=\"gemma2:2b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM},\n",
    "        {\"role\": \"user\",   \"content\": USER}, \n",
    "    ]\n",
    ")\n",
    "\n",
    "print(resp[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b71187",
   "metadata": {},
   "source": [
    "## 5. Bezpieczeństwo\n",
    "\n",
    "Nie istnieje obecnie skuteczny sposób całkowitego zabezpieczenia modeli przed zmianą ich zachowania przez użytkownika końcowego — potwierdzają to badania nad Constitutional Classifiers (https://www.anthropic.com/research/constitutional-classifiers). Dlatego systemy z LLM należy projektować tak, by nawet udany atak nie powodował poważnych skutków.\n",
    "\n",
    "Podstawowe zasady bezpieczeństwa:\n",
    "\n",
    "- Model nie może samodzielnie wykonywać decyzji biznesowych ani nieodwracalnych akcji.\n",
    "\n",
    "- Każde działanie powinno być zatwierdzane przez człowieka (Human in the Loop).\n",
    "\n",
    "- Dostępy i uprawnienia należy kontrolować programistycznie, nie poprzez prompt.\n",
    "\n",
    "- System powinien mieć jasne komunikaty, regulaminy i ograniczenia prawne.\n",
    "\n",
    "Typowe ryzyka:\n",
    "\n",
    "- Czatbot z bazą wiedzy może tworzyć błędne lub niestosowne treści.\n",
    "\n",
    "- Czatbot z dostępem do narzędzi może przypadkowo lub celowo usuwać, modyfikować lub wysyłać dane.\n",
    "  → Rozwiązanie: ogranicz uprawnienia, prowadź historię zmian i wymagaj potwierdzeń akcji.\n",
    "\n",
    "Ataki Prompt Injection mogą wynikać nie tylko ze złych intencji, lecz także z błędnej konstrukcji systemu lub nieporozumienia modelu.\n",
    "Dlatego wszystkie generowane treści należy weryfikować i zatwierdzać przez człowieka.\n",
    "\n",
    "> Celem nie jest całkowita izolacja modeli, lecz świadome ograniczanie ryzyka i kontrola efektów ich działania."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b3ba36",
   "metadata": {},
   "source": [
    "# **Walidacja promptów**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8699731",
   "metadata": {},
   "source": [
    "`PromptFoo` to narzędzie open-source do testowania, porównywania i walidacji promptów dla modeli językowych (np. ChatGPT, Ollama, Claude, Gemini, itp.).\n",
    "Pozwala automatycznie oceniać, które prompty generują najlepsze, najbardziej spójne lub najdokładniejsze odpowiedzi.\n",
    "\n",
    "Dzięki niemu możesz:\n",
    "\n",
    "- uruchamiać testy wielu promptów jednocześnie,\n",
    "\n",
    "- porównywać modele lokalne (Ollama) i zdalne (OpenAI API),\n",
    "\n",
    "- definiować własne asercje (reguły walidacji), np. „odpowiedź nie jest pusta”, „zawiera słowo kluczowe”, „ma poprawny JSON”.\n",
    "\n",
    "Instalacja PromptFoo\n",
    "1. Wymagania\n",
    "\n",
    "Node.js w wersji ≥18\n",
    "\n",
    "```bash\n",
    "node -v\n",
    "```\n",
    "\n",
    "Jeśli nie masz Node.js — pobierz go z https://nodejs.org/\n",
    "\n",
    "2. Instalacja PromptFoo\n",
    "\n",
    "Zainstaluj za pomocą npm:\n",
    "\n",
    "```bash\n",
    "npm install -g promptfoo\n",
    "```\n",
    "\n",
    "3. Sprawdzenie instalacji\n",
    "\n",
    "Po zakończeniu instalacji wpisz:\n",
    "\n",
    "```bash\n",
    "promptfoo --version\n",
    "```\n",
    "\n",
    "Jeśli pojawi się numer wersji — wszystko działa\n",
    "\n",
    "4. Utworzenie przykładowego projektu\n",
    "\n",
    "W dowolnym folderze uruchom:\n",
    "\n",
    "```bash\n",
    "promptfoo init\n",
    "```\n",
    "\n",
    "To polecenie utworzy plik konfiguracyjny promptfooconfig.yaml i przykładowe testy.\n",
    "\n",
    "5. Uruchomienie testów promptów\n",
    "\n",
    "Aby wykonać testy i zobaczyć wyniki:\n",
    "\n",
    "```bash\n",
    "promptfoo eval\n",
    "promptfoo view\n",
    "```\n",
    "\n",
    "promptfoo view otworzy interaktywny panel w przeglądarce.\n",
    "\n",
    "> Oficjalna strona: https://www.promptfoo.dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a28f3d",
   "metadata": {},
   "source": [
    "Przykładowy metaprompt do PromptFoo\n",
    "\n",
    "```bash\n",
    "<ROLE>\n",
    "Jesteś ekspertem od Prompt Engineeringu. Twoim zadaniem jest pomóc użytkownikowi stworzyć skuteczny prompt\n",
    "dla modelu językowego (LLM), który zapewni dokładne, zwięzłe i powtarzalne odpowiedzi.\n",
    "</ROLE>\n",
    "\n",
    "<GOAL>\n",
    "Na podstawie opisu zadania od użytkownika:\n",
    "1. Zidentyfikuj cel promptu i format odpowiedzi.\n",
    "2. Zaproponuj strukturę promptu (z sekcjami <ROLE>, <RULES>, <INPUT>, <OUTPUT>).\n",
    "3. Dodaj 1–2 przykłady (few-shot), które pokażą wzorzec zachowania.\n",
    "4. Zaproponuj sposób ewaluacji jego skuteczności (np. z użyciem PromptFoo).\n",
    "</GOAL>\n",
    "\n",
    "<INPUT>\n",
    "{{opis_zadania_użytkownika}}\n",
    "</INPUT>\n",
    "\n",
    "<OUTPUT>\n",
    "Zwróć gotowy prompt i krótkie uzasadnienie.\n",
    "</OUTPUT>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713788a3",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## **7. Zadania sprawdzające do sprawozdania**\n",
    "\n",
    "Do każdego zadania ułóż swoje przykłady i napisz swoje prompty.\n",
    "\n",
    "Zadanie 7.1 NER (Named Entity Recognition)\n",
    "\n",
    "Rozpoznawanie nazw własnych (osoba, organizacja, lokalizacja, data).\n",
    "Zwróć wynik w formacie JSON.\n",
    "\n",
    "przykład:\n",
    "\n",
    "```python\n",
    "SYSTEM = \"You are an information extractor.\"\n",
    "\n",
    "prompt = \"Return valid JSON listing every PERSON, LOCATION and ORGANIZATION that appears.\"\n",
    "\n",
    "text = \"\"\"7 lutego 1919, dekretem Tymczasowego Naczelnika Państwa Józefa Piłsudskiego, \n",
    "      utworzona została Pocztowa Kasa Oszczędności. Jej pierwszym dyrektorem został mianowany 28 grudnia 1919 Hubert Linde. \n",
    "      Po nim prezesami PKO byli Emil Schmidt i Henryk Gruber. Z czasem utworzono centralę banku w Warszawie \n",
    "      z siedzibą przy ul. Świętokrzyskiej 31/33 oraz pierwsze oddziały lokalne: w Krakowie, Lwowie, Łodzi, Poznaniu i Katowicach. \n",
    "      Pierwszym celem PKO stało się wprowadzenie do obiegu polskiego złotego zamiast marki polskiej (jako pochodnej \n",
    "      marki niemieckiej). Od 1920 bank posiadał osobowość prawną, jako instytucja państwowa. Pracownicy Kasy byli zrzeszeni \n",
    "      w Zrzeszeniu Pracowników Pocztowej Kasy Oszczędności, które miało swoje koła przy większych Oddziałach, np. w Warszawie, \n",
    "      w Łodzi.\"\"\"\n",
    "\n",
    "output_format = \"\"\"\n",
    "{\n",
    "  \"PERSON\": [\"<imię nazwisko>\"],\n",
    "  \"LOCATION\": [\"<miejsce>\"],\n",
    "  \"ORG\": [\"<organizacja>\"],\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='gemma2:2b',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": f\"{prompt}\\n\\nText:\\n{text}\\n\\nOutput format:\\n{output_format}\"}\n",
    "    ]\n",
    ")\n",
    "print(response['message']['content'])\n",
    "```\n",
    "---\n",
    "\n",
    "Zadanie 7.2 Analiza sentymentu\n",
    "\n",
    "Określenie tonu wypowiedzi (pozytywny, neutralny, negatywny).\n",
    "Dodaj przykład sarkazmu i porównaj interpretacje modeli.\n",
    "\n",
    "przykład: \n",
    "\n",
    "```python\n",
    "SYSTEM = \"Jesteś precyzyjnym analizatorem wydźwięku tekstu. Zwracaj wyłącznie poprawny JSON według podanej specyfikacji.\"\n",
    "\n",
    "prompt = \"\"\"Określ wydźwięk (pozytywny/negatywny/neutralny) poniższej recenzji.\n",
    "Zwróć JSON z dwoma kluczami: \"sentiment\" i \"evidence\" (zacytuj decydujący fragment, ogranicz się do trzech najbardziej emocjonalnych wyrazów).\n",
    "\n",
    "Recenzja:\n",
    "Gdy pierwszy raz uniosłam do nosa butelkę Szamponu „Lśniąca Natura”, ogarnęła mnie fala wspomnień z wakacji nad Bałtykiem – zapach morskiej bryzy splecionej z nutami słodkiej pomarańczy i soczystych malin. Już samo otwarcie opakowania było jak zdjęcie wiecznego klosza z codzienności: w jednej sekundzie łazienka zamieniła się w rozświetloną, letnią plażę, a ja – w beztroską dziewczynę z wiatrem we włosach.\n",
    "\n",
    "Gęsta, perłowa formuła wypływa z butelki niczym płynne światło. Kiedy rozprowadzam ją na wilgotnych pasmach, mam wrażenie, że każdy kosmyk wita ją z zachwytem: pianę miękką jak pianka z latte, która lekko skrzypi między palcami i otula skórę głowy kojącym chłodem. To nie jest zwykłe mycie włosów – to rytuał, w którym czuję się dopieszczona od cebulek aż po same końce.\n",
    "\n",
    "Już podczas spłukiwania słyszę charakterystyczny, czysty „skrzyp” zdrowych włosów. Strumień wody odbija światło, a moje pasma – lśniące i lekkie – tańczą w nim jak jedwabne wstążki. Nie mogę się oprzeć, by nie zanurzyć dłoni w tej tafli – gładkość rozczarowuje mnie tylko w jednym: że nie da się jej zapisać na stałe w pamięci dotyku.\n",
    "\n",
    "Po wysuszeniu czuję się, jakbym stąpała po czerwonym dywanie: pukle sypkie, sprężyste, unoszące się przy każdym ruchu głowy. Aromat, który pozostaje, przypomina delikatny perfum – dyskretny, ale wystarczająco wyrazisty, by ktoś obok zapytał z zaciekawieniem: „Czym pachniesz?” Wtedy uśmiecham się szeroko, bo wiem, że ten sekret kryje się w niewielkiej, zielonej butelce stojącej na półce.\n",
    "\n",
    "Szampon „Lśniąca Natura” to dla mnie nie tylko kosmetyk; to codzienny list miłosny do moich włosów – powiew odwagi i czułości zarazem. Jeśli Twoje pasma pragną rozgwieżdżonego blasku, a Twoje zmysły tęsknią za chwilą szczerej przyjemności, pozwól temu szamponowi szepnąć im historię o tym, jak piękno rodzi się z zachwytu.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='gemma2:2b',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "print(response['message']['content'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Zadanie 7.3 Ekstrakcja relacji\n",
    "\n",
    "Wykrywanie relacji PRACUJE_W i MIESZKA_W.\n",
    "\n",
    "przykład:\n",
    "\n",
    "```python\n",
    "SYSTEM = \"Jesteś precyzyjnym analizatorem tekstu. Znajdź wszystkie relacje gdzie osoba pracuje dla organizacji. Zwróć tablicę JSON z obiektami {\\\"person\\\":\\\"<imię>\\\",\\\"company\\\":\\\"<organizacja>\\\"}.\"\n",
    "\n",
    "shots = [\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Tekst: \\\"Jan Kowalski pracuje w Microsoft jako programista.\\\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"[{\\\"person\\\":\\\"Jan Kowalski\\\",\\\"company\\\":\\\"Microsoft\\\"}]\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tekst: \\\"Anna Kowalska teraz pracuje na AGH, ale kiedyś karierę robiła w Google.\\\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='gemma2:2b',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM},\n",
    "        *shots\n",
    "    ]\n",
    ")\n",
    "print(response['message']['content'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Zadanie 7.4 Klasyfikacja tematyczna\n",
    "\n",
    "Przypisz tekst do jednej z kategorii: nauka, sport, polityka, technologia.\n",
    "Przetestuj trzy warianty: zero-shot, few-shot, sekcyjny.\n",
    "\n",
    "```python\n",
    "SYSTEM = (\n",
    "    \"Jesteś klasyfikatorem tematów. Przypisz tekst do JEDNEJ kategorii \"\n",
    "    \"z zestawu: nauka, sport, polityka, technologia. \"\n",
    "    \"Zwróć wyłącznie JSON: {\\\"kategoria\\\":\\\"...\\\"}.\"\n",
    ")\n",
    "\n",
    "text = 'Nowy mikroskop pozwala obserwować pojedyncze atomy.'\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='gemma2:2b',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": f\"Tekst: \\\"{text}\\\"\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])\n",
    "'@ | ollama run gemma2:2b\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Zadanie 7.5 Streszczenie i parafraza\n",
    "\n",
    "Generuj dwa streszczenia (krótkie i długie) oraz parafrazę krótkiego.\n",
    "\n",
    "```bash\n",
    "@'\n",
    "<ROLE>Tworzysz streszczenia i parafrazy. Odpowiadasz w JSON.</ROLE>\n",
    "<TEXT>\n",
    "Sztuczna inteligencja wspiera naukowców w analizie danych medycznych,\n",
    "pomagając szybciej diagnozować choroby i tworzyć spersonalizowane terapie.\n",
    "</TEXT>\n",
    "<OUTPUT>\n",
    "'@ | ollama run gemma2:2b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed232e72",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Zadanie 7.6: QA z kontekstem i filtrowaniem odpowiedzi\n",
    "\n",
    "**Opis zadania:**\n",
    "\n",
    "Przygotuj prompt, który pozwoli modelowi LLM (w Ollama) odpowiadać na pytania zadane przez użytkownika, na podstawie dostarczonego kontekstu (tekstu źródłowego). Oceniane zadania:\n",
    "\n",
    "1. Zaprojektowanie struktury promptu z rolami (system/user) i sekcjami:\n",
    "   - **Kontekst**: fragment tekstu, z którego model może czerpać informacje.\n",
    "   - **Pytanie użytkownika**.\n",
    "   - **Zasady**: na przykład — „jeśli pytanie nie może być odpowiedziane z kontekstu, odpowiedz: ‘Brak wystarczających informacji’”, „podaj źródło w nawiasach [] jeśli to możliwe”.\n",
    "2. Przeprowadzenie kilku eksperymentów z wariantami promptu:\n",
    "   - wariant *bez zasad*,\n",
    "   - wariant *ze ścisłymi zasadami*,\n",
    "   - wariant z **few-shot** przykładem (kontekst + pytanie + przykładowa poprawna odpowiedź).\n",
    "3. Dla zadanych par (kontekst + pytanie) porównać odpowiedzi z różnych wariantów promptów, ocenić poprawność, rozbieżności i halucynacje.\n",
    "4. (Dodatkowe) napisać testy w `PromptFoo`, by automatycznie sprawdzać, czy:\n",
    "   - odpowiedź nie jest pusta,\n",
    "   - jeśli pytanie dotyczy czegoś niezawartego w kontekście, to model faktycznie odpowiada „Brak wystarczających informacji” (lub inny ustalony komunikat).\n",
    "\n",
    "**Przykład formatu (schemat):**\n",
    "\n",
    "```\n",
    "<system>\n",
    "Jesteś asystentem, który odpowiada na pytania na podstawie dostarczonego tekstu. Jeśli pytanie wykracza poza tekst, odpowiedz „Brak wystarczających informacji”.\n",
    "</system>\n",
    "\n",
    "<kontekst>\n",
    "{tutaj wklej fragment tekstu}\n",
    "</kontekst>\n",
    "\n",
    "<user>\n",
    "Pytanie: {tutaj pytanie}\n",
    "</user>\n",
    "```\n",
    "\n",
    "**Materiały do testów:**\n",
    "\n",
    "Przygotuj:\n",
    "\n",
    "- pytanie, na które odpowiedź jest w tekście,\n",
    "- pytanie spoza tekstu.\n",
    "\n",
    "------\n",
    "\n",
    "Zadanie 7.7: Porównanie strategii promptów w klasyfikacji wieloklasowej + analiza\n",
    "\n",
    "**Opis zadania:**\n",
    "\n",
    "Celem jest zbadanie i porównanie efektywności różnych strategii promptów (zero-shot, few-shot, oddzielone sekcjami) w zadaniu klasyfikacji wieloklasowej tekstu, a następnie analiza wyników (precision / recall / błędy) i wyciągnięcie wniosków.\n",
    "\n",
    "1. Przygotuj zbiór testowy (np. 20 krótkich zdań) i zbiór klas (np. 4–5 tematów: „technologia”, „sport”, „zdrowie”, „polityka”).\n",
    "2. Zaimplementuj co najmniej trzy warianty promptu:\n",
    "   - **Zero-shot**: zadanie z instrukcją i klasami, bez przykładów.\n",
    "   - **Few-shot**: dwa/trzy przykłady (tekst + klasa) w promptcie, a potem nowe wejścia.\n",
    "   - **Sekcje / separatory**: z oddzieloną sekcją „Zasady”, „Dane wejściowe”, „Wynik” – np. strukturą RULES, INPUT, OUTPUT.\n",
    "3. Uruchom klasyfikację na wszystkich wariantach promptów (ten sam model i parametry), zbierz odpowiedzi.\n",
    "4. Zanalizuj wyniki:\n",
    "   - policz *accuracy*, *konfuzje między klasami* (które klasy najczęściej pomylono),\n",
    "   - sprawdź, dla których przykładów różne promptowania dały różne odpowiedzi – spróbuj zdiagnozować, dlaczego.\n",
    "   - opcjonalnie: zmodyfikuj prompt (np. zmiana kolejności instrukcji, dodanie więcej przykładów) i sprawdź, czy następuje poprawa.\n",
    "5. (Bonus) Zintegruj testy w `PromptFoo`, by automatycznie weryfikować trafność klasyfikacji (np. asercje *equals(expected_class)* dla części przypadków).\n",
    "\n",
    "------\n",
    "\n",
    "Zadanie 7.8: Mini-projekt: Asystent planowania wyjazdu\n",
    "\n",
    "Stwórz **asystenta turystycznego**, który:\n",
    "\n",
    "1. Przyjmuje nazwę miasta w Polsce,\n",
    "2. Zwraca plan jednodniowego zwiedzania (poranne, popołudniowe, wieczorne atrakcje),\n",
    "3. Uwzględnia ograniczenie budżetu przekazane przez użytkownika,\n",
    "4. Zwraca wynik w formacie Markdown z nagłówkami `### Rano`, `### Popołudnie`, `### Wieczór`.\n",
    "\n",
    "## **Dalsze materiały**\n",
    "\n",
    "- Gandalf (https://gandalf.lakera.ai/baseline)\n",
    "- Ollama Docs (https://docs.ollama.com/)\n",
    "- Prompt Engineering Guide (https://www.promptingguide.ai/)\n",
    "- PromptFoo / LangFuse (https://www.promptfoo.dev/) / (https://langfuse.com/)\n",
    "- Anthropic Research (https://www.anthropic.com/research)\n",
    "- Publikacje o CoT, Self-Critique i Prompt Injection\n",
    "\n",
    "> **Refleksja:** Która technika promptowania dała najbardziej stabilne rezultaty i jak można zoptymalizować proces w kolejnych projektach?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
